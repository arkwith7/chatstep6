{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기사 제목을 가지고 긍정 / 부정 / 중립으로 분류하는 모델 만들어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 학습데이터, 테스트데이터 만들기\n",
    "먼저 모델을 만들고 나서 학습을 시킬 데이터를 만들기 위해 네이버에서 몇몇의 기업을 선정하고\n",
    "\n",
    "그 기업에 대한 기사의 제목을 크롤링하였습니다.\n",
    "\n",
    " \n",
    "\n",
    "크롤링 후에 학습시 필요한 긍정, 부정, 중립을 나타내는 label이 있었어야했는데\n",
    "\n",
    "손으로 일일이 부정, 긍정, 중립 이 세 가지로 label을 붙이려고 하다보니\n",
    "\n",
    "눈도 아프고 비슷한 내용임에도 불구하고 앞에서는 긍정으로 했다가 뒤에서는 중립으로 표기하는 등의 문제가 있었습니다.\n",
    "\n",
    " \n",
    "\n",
    "이를 컴퓨터가 긍정적인 단어, 부정적인 단어가 포함되어있는지 여부를 확인하여\n",
    "\n",
    "자동으로 라벨을 붙여주면 편할 것 같아 그렇게 만들어 보았습니다.\n",
    "\n",
    "\n",
    "[negative_words_self.txt](data/negative_words_self.txt)\n",
    "\n",
    "\n",
    "[positive_words_self.txt](positive_words_self.txt)\n",
    "\n",
    "\n",
    "\n",
    "먼저, 긍정적인 단어, 부정적인 단어가 포함된 txt파일을 각각 만들어주었습니다.\n",
    "\n",
    "뉴스 기사를 보며 만든 긍정적인 단어, 부정적인 단어 모음입니다.\n",
    "\n",
    "단어는 생각나는대로 계속 추가하고자합니다.\n",
    "\n",
    " \n",
    "\n",
    "코드에서는 이 단어들을 파일에서 positive, negative라는 list로 받아와서\n",
    "\n",
    "두 개의 list를 합쳐 posneg라는 list를 만들고 크롤링해오는 단어에서 posneg안에 있는 단어가 포함되어있으면 긍정, 부정 라벨을 붙여주고\n",
    "\n",
    "포함되어있지 않으면 그냥 중립인 0의 상태로 그대로 두도록 만들어 보았습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 단어를 불러와 posneg리스트를 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs \n",
    "\n",
    "positive = [] \n",
    "negative = [] \n",
    "posneg = []\n",
    "\n",
    "pos = codecs.open(\"data/positive_words_self.txt\", 'rb', encoding='UTF-8')\n",
    "\n",
    "while True:\n",
    "    line = pos.readline() \n",
    "    line = line.replace('\\n', '') \n",
    "    positive.append(line) \n",
    "    posneg.append(line) \n",
    "    \n",
    "    if not line: \n",
    "        break \n",
    "        \n",
    "pos.close()\n",
    "\n",
    "neg = codecs.open(\"data/negative_words_self.txt\", 'rb', encoding='UTF-8') \n",
    "\n",
    "while True: \n",
    "    line = neg.readline() \n",
    "    line = line.replace('\\n', '') \n",
    "    negative.append(line) \n",
    "    posneg.append(line) \n",
    "    \n",
    "    if not line: break \n",
    "        \n",
    "neg.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링한 기사 제목과 기사 제목과 posneg를 활용하여 만든 긍정(1), 부정(-1), 중립(0)라벨 정보를 가지는 dataframe을 만드는 함수\n",
    "\n",
    "(예시 : 네이버에서 버거킹으로 검색하여 나온 기사 4,000개 제목과 각각 제목의 긍정, 부정, 중립 라벨 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 버거킹 햄버거 2개 8000원 행사\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 버거킹의 검은빵 버거는 끼리끼리 시너지\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 버거킹 어떤 버거를 선택해도 2개에 8000원\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 버거킹 킹스초이스King’s Choice 프로모션 실시\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 침체됐던 햄버거시장 MZ세대 공략으로 회복세\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 위기가 기회코로나 뚫고 사업 확장하는 프랜차이즈\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 코로나19 시대 배달 음식은 일상\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 먹템 흑맥주향 1도 안나네 짠맛 강한 버거킹 기네스와퍼\n",
      "15 positive? 테스트 :  38 비교단어 :  출시 인덱스 :  15 버거킹 더 맛있게 돌아온 미국 남부의 매콤한 맛 뉴올리언스 치킨버거 출시\n",
      "46 negative? 테스트 :  0 비교단어 :   인덱스 :  46 버거킹이 헬싱키에서 맥도날드와의 사랑을 선언하다\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f19284cc14ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mmy_title_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mmy_title_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_title_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2_env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "import pandas as pd \n",
    "\n",
    "label = [0] * 4000 \n",
    "my_title_dic = {\"title\":[], \"label\":label} \n",
    "\n",
    "j = 0 \n",
    "\n",
    "for i in range(400): \n",
    "    num = i * 10 + 1 \n",
    "    # bhc # url = \"https://search.naver.com/search.naver?&where=news&query=bhc&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=23&start=\" + str(num) \n",
    "    # 아오리라멘 \n",
    "    # url2 = \"https://search.naver.com/search.naver?&where=news&query=%EC%95%84%EC%98%A4%EB%A6%AC%EB%9D%BC%EB%A9%98&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=34&start=\" + str(num) \n",
    "    \n",
    "    # 버거킹 \n",
    "    url3 = \"https://search.naver.com/search.naver?&where=news&query=%EB%B2%84%EA%B1%B0%ED%82%B9&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=23&start=\" + str(num) \n",
    "    \n",
    "    req = requests.get(url3) \n",
    "    \n",
    "    soup = BeautifulSoup(req.text, 'lxml') \n",
    "    titles = soup.select(\"a._sp_each_title\") \n",
    "    \n",
    "    for title in titles:\n",
    "        \n",
    "        title_data = title.text \n",
    "        title_data = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…\\\"\\“》]', '', title_data) \n",
    "        my_title_dic['title'].append(title_data) \n",
    "        \n",
    "        for i in range(len(posneg)): \n",
    "            posflag = False \n",
    "            negflag = False \n",
    "            \n",
    "            if i < (len(positive)-1): \n",
    "                \n",
    "                # print(title_data.find(posneg[i])) \n",
    "                if title_data.find(posneg[i]) != -1: \n",
    "                    posflag = True \n",
    "                    print(i, \"positive?\",\"테스트 : \",title_data.find(posneg[i]),\"비교단어 : \", posneg[i], \"인덱스 : \", i, title_data) \n",
    "                    break \n",
    "                    \n",
    "            if i > (len(positive)-2): \n",
    "                \n",
    "                if title_data.find(posneg[i]) != -1: \n",
    "                    negflag = True \n",
    "                    print(i, \"negative?\",\"테스트 : \",title_data.find(posneg[i]),\"비교단어 : \", posneg[i], \"인덱스 : \", i, title_data) \n",
    "                    break \n",
    "                    \n",
    "        if posflag == True: \n",
    "            label[j] = 1 \n",
    "            # print(\"positive\", j) \n",
    "        elif negflag == True: \n",
    "            label[j] = -1 \n",
    "            # print(\"negative\", j) \n",
    "        elif negflag == False and posflag == False: \n",
    "            label[j] = 0 \n",
    "            # print(\"objective\", j) \n",
    "            \n",
    "        j = j + 1 \n",
    "        \n",
    "    my_title_dic['label'] = label \n",
    "    my_title_df = pd.DataFrame(my_title_dic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MOU',\n",
       " '제휴',\n",
       " '주목',\n",
       " '호응',\n",
       " '돌파',\n",
       " '이목',\n",
       " '수상',\n",
       " '입점',\n",
       " '인기',\n",
       " '열풍',\n",
       " '진화',\n",
       " '대박',\n",
       " '순항',\n",
       " '유치',\n",
       " '1위',\n",
       " '출시',\n",
       " '선보여',\n",
       " '오픈',\n",
       " '팝업',\n",
       " '돌풍',\n",
       " '팝업스토어',\n",
       " '인싸',\n",
       " '줄서서',\n",
       " '인기',\n",
       " '대세',\n",
       " '트렌드',\n",
       " '불티',\n",
       " '진출',\n",
       " '부상',\n",
       " '체결',\n",
       " '증가',\n",
       " '봉사',\n",
       " '기부',\n",
       " '신메뉴',\n",
       " '신제품',\n",
       " '신상',\n",
       " '최고',\n",
       " '새로운',\n",
       " '편한',\n",
       " '미소',\n",
       " '맛집',\n",
       " '착한가게',\n",
       " '캠패인',\n",
       " '순항',\n",
       " '착한',\n",
       " '제작지원',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
