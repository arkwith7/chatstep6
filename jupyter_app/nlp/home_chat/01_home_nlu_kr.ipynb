{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 2018. 2. 14.\n",
    "\n",
    "@author: phs\n",
    "'''\n",
    "\"\"\"\n",
    "1.대화 말뭉치 파일을 읽어들인다.\n",
    "2.대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "3.Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "4.딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "5.자연어 이해 모델을 관리한다(저장,읽기)\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# things we need for NLP\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "# komoran = Komoran()\n",
    "twitter = Okt()\n",
    "\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dialog_intents_jsonfile(input_file_name):\n",
    "    \"\"\"\n",
    "     대화 말뭉치 파일을 읽어들인다.\n",
    "    \"\"\"\n",
    "    with open(input_file_name, 'rt', encoding='UTF8') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "        \n",
    "    return intents\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialog_nlp_processing(intents):\n",
    "    \"\"\"\n",
    "     대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_words = ['?']\n",
    "    # loop through each sentence in our intents patterns\n",
    "    for intent in intents['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each word in the sentence\n",
    "#             w = nltk.word_tokenize(pattern)\n",
    "            pos_result = twitter.pos(pattern, norm=True, stem=True)\n",
    "            w = [lex for lex, pos in pos_result]\n",
    "            # add to our words list\n",
    "            words.extend(w)\n",
    "            # add to documents in our corpus\n",
    "            documents.append((w, intent['tag']))\n",
    "            # add to our classes list\n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])\n",
    "    \n",
    "    # stem and lower each word and remove duplicates\n",
    "#     words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "#     words = sorted(list(set(words)))\n",
    "    words = [w for w in words if w not in ignore_words]\n",
    "    words = sorted(list(set(words)))\n",
    "    \n",
    "    # remove duplicates\n",
    "    classes = sorted(list(set(classes)))\n",
    "    \n",
    "    return classes, documents, words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_machine_learning(classes, documents, words):\n",
    "    \"\"\"\n",
    "    Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "    \"\"\"\n",
    "    \n",
    "    # create our training data\n",
    "    training = []\n",
    "    output_row = []\n",
    "    # create an empty array for our output\n",
    "    output_empty = [0] * len(classes)\n",
    "    \n",
    "    # training set, bag of words for each sentence\n",
    "    for doc in documents:\n",
    "        # initialize our bag of words\n",
    "        bag = []\n",
    "        # list of tokenized words for the pattern\n",
    "        pattern_words = doc[0]\n",
    "        # stem each word\n",
    "#         pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "        # create our bag of words array\n",
    "        for w in words:\n",
    "            bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "        # output is a '0' for each tag and '1' for current tag\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "        training.append([bag, output_row])\n",
    "    \n",
    "    # shuffle our features and turn into np.array\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training)\n",
    "    \n",
    "    # create train and test lists\n",
    "    train_x = list(training[:,0])\n",
    "    train_y = list(training[:,1])\n",
    "\n",
    "    return train_x, train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tensorflow_learning_model(train_x, train_y, output_model_file_name):\n",
    "    \"\"\"\n",
    "    딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "    \"\"\"\n",
    "    \n",
    "    # reset underlying graph data\n",
    "    #tf.compat.v1.reset_default_graph()\n",
    "    tf.reset_default_graph()\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "    \n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(net, tensorboard_dir='home_tflearn_kr_logs')\n",
    "    # Start training (apply gradient descent algorithm)\n",
    "    model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "\n",
    "#     # Using Keras, Build neural network\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Dense(8, input_shape=(len(train_x[0]),)),\n",
    "#         tf.keras.layers.Dense(8),\n",
    "#         tf.keras.layers.Dense(len(train_y[0]), activation=\"softmax\"),\n",
    "#         ])\n",
    "    \n",
    "#     model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#     model.fit(train_x, train_y, epochs=1000, batch_size=8)    \n",
    "\n",
    "    # save the trained model to directory\n",
    "    model.save(output_model_file_name)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "    sentence_words = [lex for lex, pos in pos_result]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "def save_training_data_structures(words, classes, train_x, train_y, output_training_data_file_name):\n",
    "    \"\"\"\n",
    "    자연어 이해 모델을 관리한다(저장,읽기)\n",
    "    \"\"\"\n",
    "    # save all of our data structures\n",
    "    pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( output_training_data_file_name, \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 documents\n",
      "11 classes ['Slang', 'goodbye', 'greeting', 'thanks', '경력', '약점', '어학', '자격', '자기소개', '장점', '학력']\n",
      "77 unique stemmed words ['.', '가', '감사', '감사하다', '개발', '개선', '거기', '경력', '경험', '계세', '계시다', '고맙다', '과', '그렇다', '꺼지다', '나중', '날', '누구', '는', '다음', '닥치다', '돈', '들리다', '또', '를', '많이', '멍청이', '무슨', '바보', '버세', '번', '보완', '부분', '부족', '부탁', '사라지다', '사항', '새끼', '서비스', '소개', '수고', '안녕', '안녕하다', '야', '약점', '어떻다', '어학', '언제', '업적', '에', '에요', '여기', '영어', '요', '은', '을', '이', '이다', '일본어', '임마', '있다', '자기', '자다', '자신', '장사', '장점', '전공', '제품', '졸업', '좋다', '줄다', '중국어', '지내다', '친절하다', '하다', '학교', '해주다']\n"
     ]
    }
   ],
   "source": [
    "# 대화 말뭉치 파일을 읽어들인다.\n",
    "input_file_name = './DialogIntents/intents_home_kr.json'\n",
    "intents = read_dialog_intents_jsonfile(input_file_name)\n",
    "    \n",
    "# 대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "classes, documents, words = dialog_nlp_processing(intents)\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\ipykernel_launcher.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "train_x, train_y = prepare_machine_learning(classes, documents, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "77\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(len(train_x[0]))\n",
    "print(train_y[0])\n",
    "print(len(train_y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6999  | total loss: \u001b[1m\u001b[32m0.10495\u001b[0m\u001b[0m | time: 0.018s\n",
      "| Adam | epoch: 1000 | loss: 0.10495 - acc: 0.9561 -- iter: 48/54\n",
      "Training Step: 7000  | total loss: \u001b[1m\u001b[32m0.09561\u001b[0m\u001b[0m | time: 0.020s\n",
      "| Adam | epoch: 1000 | loss: 0.09561 - acc: 0.9605 -- iter: 54/54\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\saint\\GroupProject\\step5\\jupyter_app\\nlp\\home_chat\\NLUModel\\model_home_kr.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "output_model_file_name = './NLUModel/model_home_kr.tflearn'\n",
    "model = create_tensorflow_learning_model(train_x, train_y, output_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 자연어 이해 모델을 관리한다(저장,읽기)\n",
    "output_training_data_file_name = \"./NLUModel/training_data_home_kr\"\n",
    "save_training_data_structures(words, classes, train_x, train_y, output_training_data_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is Bag of word for '비젼은 무엇입니까?' :[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "classes :['Slang', 'goodbye', 'greeting', 'thanks', '경력', '약점', '어학', '자격', '자기소개', '장점', '학력']\n",
      "model.predict([p]) :[[4.2046016e-11 4.0230997e-08 1.4021193e-12 9.6152912e-11 2.9819619e-04\n",
      "  2.9969469e-01 6.5913968e-02 1.6836681e-05 4.1041709e-03 6.2702901e-07\n",
      "  6.2997150e-01]]\n",
      "54 documents\n",
      "11 classes ['Slang', 'goodbye', 'greeting', 'thanks', '경력', '약점', '어학', '자격', '자기소개', '장점', '학력']\n",
      "77 unique stemmed words ['.', '가', '감사', '감사하다', '개발', '개선', '거기', '경력', '경험', '계세', '계시다', '고맙다', '과', '그렇다', '꺼지다', '나중', '날', '누구', '는', '다음', '닥치다', '돈', '들리다', '또', '를', '많이', '멍청이', '무슨', '바보', '버세', '번', '보완', '부분', '부족', '부탁', '사라지다', '사항', '새끼', '서비스', '소개', '수고', '안녕', '안녕하다', '야', '약점', '어떻다', '어학', '언제', '업적', '에', '에요', '여기', '영어', '요', '은', '을', '이', '이다', '일본어', '임마', '있다', '자기', '자다', '자신', '장사', '장점', '전공', '제품', '졸업', '좋다', '줄다', '중국어', '지내다', '친절하다', '하다', '학교', '해주다']\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"비젼은 무엇입니까?\", words)\n",
    "print(\"p is Bag of word for '비젼은 무엇입니까?' :{}\".format(p))\n",
    "print(\"classes :{}\".format(classes))\n",
    "    \n",
    "print(\"model.predict([p]) :{}\".format(model.predict([p])))\n",
    "    \n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatApp_env",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
