{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 2018. 2. 14.\n",
    "\n",
    "@author: phs\n",
    "\n",
    "1. 대화 말뭉치 파일을 읽어들인다.\n",
    "2. 딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어 들여서 반환한다.\n",
    "3. 딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들여서 반환한다.\n",
    "4. 입력받은 문장을 자연어 처리한다(토크나이저, 스태밍).\n",
    "5. 자연어 처리한 문장을 Bag of word 생성하여 반환한다.\n",
    "6. 입력받은 문장을 학습 모델(자연어 이해 모델)을 이용 분류하여  결과를 반환한다.\n",
    "7. 분류된 말뭉치 대화에서 임의로 한 문장을 선택하여 입력받은 문장의 대답으로 반혼한다.\n",
    "\n",
    "'''\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# things we need for NLP\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "# komoran = Komoran()\n",
    "twitter = Okt()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "def read_dialog_intents_jsonfile(input_file_name):\n",
    "    \"\"\"\n",
    "     대화 말뭉치 파일을 읽어들인다.\n",
    "    \"\"\"\n",
    "    with open(input_file_name, 'rt', encoding='UTF8') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "        \n",
    "    return intents\n",
    "\n",
    "# 대화 말뭉치와 대화 의도가 정의된 JSON 문서 집합 읽기\n",
    "input_file_name = './DialogIntents/intents_home_kr.json'\n",
    "intents = read_dialog_intents_jsonfile(input_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "def restore_training_data_structures(input_training_data_file_name):\n",
    "    \"\"\"\n",
    "     딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어 들여서 반환한다.\n",
    "    \"\"\"\n",
    "    # restore all of our data structures\n",
    "    data = pickle.load( open( input_training_data_file_name, \"rb\" ) )\n",
    "    words = data['words']\n",
    "    classes = data['classes']\n",
    "    train_x = data['train_x']\n",
    "    train_y = data['train_y']\n",
    "    \n",
    "    return classes, words, train_x, train_y\n",
    "\n",
    "#  딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어들임\n",
    "input_training_data_file_name = \"./NLUModel/training_data_home_kr_tf2\"\n",
    "classes, words, train_x, train_y = restore_training_data_structures(input_training_data_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 개의 train_x\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] data of train_x[0]\n",
      "54 개의 train_y\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] data of train_y[0]\n",
      "11 classes ['Slang', 'goodbye', 'greeting', 'thanks', '경력', '약점', '어학', '자격', '자기소개', '장점', '학력']\n",
      "77 unique stemmed words ['.', '가', '감사', '감사하다', '개발', '개선', '거기', '경력', '경험', '계세', '계시다', '고맙다', '과', '그렇다', '꺼지다', '나중', '날', '누구', '는', '다음', '닥치다', '돈', '들리다', '또', '를', '많이', '멍청이', '무슨', '바보', '버세', '번', '보완', '부분', '부족', '부탁', '사라지다', '사항', '새끼', '서비스', '소개', '수고', '안녕', '안녕하다', '야', '약점', '어떻다', '어학', '언제', '업적', '에', '에요', '여기', '영어', '요', '은', '을', '이', '이다', '일본어', '임마', '있다', '자기', '자다', '자신', '장사', '장점', '전공', '제품', '졸업', '좋다', '줄다', '중국어', '지내다', '친절하다', '하다', '학교', '해주다']\n"
     ]
    }
   ],
   "source": [
    "print (len(train_x), \"개의 train_x\")\n",
    "print (train_x[0], \"data of train_x[0]\")\n",
    "print (len(train_y), \"개의 train_y\")\n",
    "print (train_y[0], \"data of train_y[0]\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our saved model\n",
    "tf2_model_file_name = './NLUModel/model_home_kr_tf2'\n",
    "model = tf.keras.models.load_model(tf2_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# things we need for Tensorflow\n",
    "\n",
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.70\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "#     print(\"sentence pos_result[%s]\" % pos_result)\n",
    "#     print(\"sentence pos_result[0][0][%s]\" % pos_result[0][0])\n",
    "#     print(\"sentence pos_result[1][0][%s]\" % pos_result[1][0])\n",
    "#     print(\"sentence pos_result[][%s]\" % pos_result[0])\n",
    "    sentence_words = [lex for lex, pos in pos_result]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "#     return(np.array(bag))\n",
    "    return bag\n",
    "\n",
    "\n",
    "def classify(sentence, words, classes, model):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, intents, words, classes, model, userID='123', show_details=False):\n",
    "    results = classify(sentence, words, classes, model)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return random.choice(i['responses'])\n",
    "\n",
    "            results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['안녕',\n",
       "    '안녕하세요',\n",
       "    '잘 지냈어요?',\n",
       "    '여기요',\n",
       "    '잘 계셨어요?',\n",
       "    '거기 누구 있어요?',\n",
       "    '계세요',\n",
       "    '계셔요',\n",
       "    '좋은 날 입니다'],\n",
       "   'responses': ['안녕하세요, 방문해 주셔서 감사합니다.',\n",
       "    '다시 만나서 반가워요.',\n",
       "    '만나서 반갑습니다',\n",
       "    '찾아 주셔서 감사합니다',\n",
       "    '안녕하세요, 어떻게 도와 드릴까요?'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['안녕히 계셔요.',\n",
       "    '그럼 나중에 또 들릴께요',\n",
       "    '나중에요',\n",
       "    '다음에',\n",
       "    '다음번에',\n",
       "    '장사 잘 하세요',\n",
       "    '돈 많이 버세요',\n",
       "    '수고하세요',\n",
       "    '친절한 서비스에 감사합니다.'],\n",
       "   'responses': ['나중에 또 오세요, 방문해 주셔서 감사합니다.',\n",
       "    '좋은 하루 되세요.',\n",
       "    '안녕히 가세요',\n",
       "    '감사합니다.',\n",
       "    '고맙습니다',\n",
       "    '즐거운 시간 되세요']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['감사', '감사합니다', '고맙습니다', '좋은 서비스 감사합니다'],\n",
       "   'responses': ['감사합니다']},\n",
       "  {'tag': 'Slang',\n",
       "   'patterns': ['멍청이', '바보', '야임마', '꺼져', '사라져', '새끼', '닥쳐'],\n",
       "   'responses': ['좋은 말로 해주세요.',\n",
       "    '듣기가 거북하네요',\n",
       "    '앞으로 더 노력 하겠습니다.',\n",
       "    '좋은 모습으로 만나 뵐 수 있기를 바랍니다.']},\n",
       "  {'tag': '자기소개',\n",
       "   'patterns': ['자기자신 소개를 해주세요?', '자기자신 소개를 부탁합니다?'],\n",
       "   'responses': ['저는 서울에 사는 홍길동이라고 합니다. 캐글 대학에서 자연어 처리를 전공하였고 재학중 자연어처리 기반 테이터사이언스 경진 대회에서 참가하여 수상 실적이 있습니다']},\n",
       "  {'tag': '경력',\n",
       "   'patterns': ['경력', '업적은?', '무슨 개발 경험이 있나요?', '경력을 소개해 주세요'],\n",
       "   'responses': ['캐글 경진대회 데이터사이언스 수상 실적이 있습니다'],\n",
       "   'context_set': 'serviceTechnology'},\n",
       "  {'tag': '학력',\n",
       "   'patterns': ['학교졸업', '언제 졸업 했나요?', '전공은'],\n",
       "   'responses': ['캐글대학 빅데이터처리학과를 2020년 2월에 졸업하였습니다.'],\n",
       "   'context_set': 'serviceTechnology'},\n",
       "  {'tag': '어학',\n",
       "   'patterns': ['영어는', '어학은', '중국어는', '일본어는'],\n",
       "   'responses': ['영어, 중국어, 일본어 회화가 가능합니다.'],\n",
       "   'context_set': 'serviceTechnology'},\n",
       "  {'tag': '자격',\n",
       "   'patterns': ['서비스', '서비스는?', '어떤 서비스가 있나요?', '제품과 서비스는?'],\n",
       "   'responses': ['정보처리기사 자격증과 중국어 신HSK5급 자격이 있습니다.'],\n",
       "   'context_set': 'serviceTechnology'},\n",
       "  {'tag': '약점',\n",
       "   'patterns': ['약점은', '보완 사항은?', '개선해야 할 부분은', '부족'],\n",
       "   'responses': ['일을 빨리 열심히 많이 하는것입니다.',\n",
       "    '일을 너무 신중하게 하는것입니다.',\n",
       "    '한가지를 시키면 두가지를 하는것입니다'],\n",
       "   'context_set': 'serviceTechnology'},\n",
       "  {'tag': '장점',\n",
       "   'patterns': ['장점', '잘 하는것은?', '어떤 서비스가 있나요?', '제품과 서비스는?'],\n",
       "   'responses': ['건강하고 설실합니다', '항상 미소를 잃지 않습니다.'],\n",
       "   'context_filter': 'serviceTechnology'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('안녕?')[[('greeting', 0.9997557)]]\n",
      "response('안녕?') ==> [다시 만나서 반가워요.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '안녕?'\n",
    "print(\"classify('안녕?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('안녕?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('자기자신 소개를 부탁합니다?')[[('자기소개', 0.9998404)]]\n",
      "response('자기자신 소개를 부탁합니다?') ==> [저는 서울에 사는 홍길동이라고 합니다. 캐글 대학에서 자연어 처리를 전공하였고 재학중 자연어처리 기반 테이터사이언스 경진 대회에서 참가하여 수상 실적이 있습니다]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '자기자신 소개를 부탁합니다?'\n",
    "print(\"classify('자기자신 소개를 부탁합니다?')[%s]\" % classify(sentence, words, classes, model))\n",
    "print(\"response('자기자신 소개를 부탁합니다?') ==> [%s]\" % response(sentence, intents, words, classes, model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('보완 사항은?')[[('약점', 0.9999044)]]\n",
      "response('보완 사항은?') ==> [일을 너무 신중하게 하는것입니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '보완 사항은?'\n",
    "print(\"classify('보완 사항은?')[%s]\" % classify(sentence, words, classes, model))\n",
    "print(\"response('보완 사항은?') ==> [%s]\" % response(sentence, intents, words, classes, model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('잘 하는것은?')[[('장점', 0.9981375)]]\n",
      "response('잘 하는것은?') ==> [건강하고 설실합니다]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '잘 하는것은?'\n",
    "print(\"classify('잘 하는것은?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('잘 하는것은?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "tag: greeting\n",
      "response('안녕?') ==> [찾아 주셔서 감사합니다]\n",
      "classify('안녕?')[[('greeting', 0.9997557)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear context\n",
    "#response(\"Hi there!\", show_details=True)\n",
    "sentence = '안녕'\n",
    "print(\"response('안녕?') ==> [{}]\".format(response(sentence, intents, words, classes, model,show_details=True)))\n",
    "print(\"classify('안녕?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('학교졸업')[[('학력', 0.99988914)]]\n",
      "response('학교졸업') ==> [캐글대학 빅데이터처리학과를 2020년 2월에 졸업하였습니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '학교졸업'\n",
    "print(\"classify('학교졸업')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('학교졸업') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('안녕히 계셔요.')[[('goodbye', 0.9997837)]]\n",
      "response('안녕히 계셔요.') ==> [즐거운 시간 되세요]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '안녕히 계셔요.'\n",
    "print(\"classify('안녕히 계셔요.')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('안녕히 계셔요.') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('뭐야?')[[('Slang', 0.9894112)]]\n",
      "response('뭐야?') ==> [앞으로 더 노력 하겠습니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '뭐야?'\n",
    "print(\"classify('뭐야?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('뭐야?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
